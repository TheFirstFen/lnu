# Instuderingsfr√•gor och Svar

1. **Skriv ett program i Go som skapar en goroutine som skriver ut en text p√• sk√§rmen**
    ```go
    package main
    import (
        "fmt"
        "time"
    )
    func main() {
        go func() {
            fmt.Println("Hello from goroutine")
        }()
        time.Sleep(1 * time.Second) // V√§nta s√• att goroutinen hinner k√∂ra
    }
    ```

2. **Skriv ett program i Go som l√§ser ett v√§rde fr√•n en kanal och skriver ut det p√• sk√§rmen**
    ```go
    package main
    import "fmt"
    func main() {
        ch := make(chan string)
        go func() {
            ch <- "Hello from channel"
        }()
        message := <-ch
        fmt.Println(message)
    }
    ```

3. **Var √§r GIL i Python och hur p√•verkar det tr√•dade program i Python?**
    GIL, eller Global Interpreter Lock, √§r en mekanism i CPython interpretern som begr√§nsar exekveringen av tr√•dar till en √•t g√•ngen. Detta p√•verkar tr√•dade program genom att f√∂rhindra dem fr√•n att fullt ut utnyttja flerk√§rniga processorer i situationer d√§r tr√•darna utf√∂r CPU-bound arbete.

4. **Skriv ett program i Go som kan ge upphov till ett data race**
    ```go
    package main
    import (
        "fmt"
        "sync"
    )
    var counter int
    func main() {
        var wg sync.WaitGroup
        for i := 0; i < 1000; i++ {
            wg.Add(1)
            go func() {
                counter++
                wg.Done()
            }()
        }
        wg.Wait()
        fmt.Println("Counter:", counter)
    }
    ```

5. **Implementera Peterson‚Äôs algorithm i Go-liknande kod.**
    ```go
    package main
    import (
        "fmt"
        "time"
    )
    var turn int
    var flag = []bool{false, false}
    func petersonsAlgorithm(threadID int) {
        other := 1 - threadID
        flag[threadID] = true
        turn = other
        for flag[other] && turn == other {
            // V√§nta
        }
        // Kritisk sektion
        fmt.Printf("Thread %d is in the critical section\n", threadID)
        time.Sleep(1 * time.Second) // Simulerar arbete i den kritiska sektionen
        flag[threadID] = false
    }
    func main() {
        go petersonsAlgorithm(0)
        go petersonsAlgorithm(1)
        time.Sleep(3 * time.Second) // V√§nta s√• att b√•da gorutinerna hinner k√∂ra
    }
    ```
6. **Implementera en producent i Go-liknande kod som anv√§nder mutex f√∂r √∂msesidig uteslutning**
    ```go
    package main
    import (
        "fmt"
        "sync"
        "time"
    )
    var mutex sync.Mutex
    var count int
    func producer() {
        for i := 0; i < 5; i++ {
            mutex.Lock()
            count = count + 1
            fmt.Println("Producer produced:", count)
            mutex.Unlock()
            time.Sleep(1 * time.Second)
        }
    }
    func main() {
        go producer()
        time.Sleep(6 * time.Second) // V√§nta s√• att producenten hinner producera
    }
    ```

7. **Implementera en producent i Go-liknande kod som anv√§nder kanaler f√∂r √∂msesidig uteslutning**
    ```go
    package main
    import (
        "fmt"
        "time"
    )
    func producer(ch chan<- int) {
        for i := 0; i < 5; i++ {
            ch <- i
            fmt.Println("Produced:", i)
            time.Sleep(1 * time.Second)
        }
        close(ch)
    }
    func main() {
        ch := make(chan int)
        go producer(ch)
        for value := range ch {
            fmt.Println("Consumed:", value)
        }
    }
    ```

8. **Ge ett exempel i Go d√§r tv√• goroutines delar en variabel och anv√§nder en mutex f√∂r att (korrekt) skydda tillg√•ngen**
    ```go
    package main
    import (
        "fmt"
        "sync"
    )
    var wg sync.WaitGroup
    var mutex sync.Mutex
    var sharedVar int
    func increment() {
        mutex.Lock()
        sharedVar++
        mutex.Unlock()
        wg.Done()
    }
    func main() {
        for i := 0; i < 100; i++ {
            wg.Add(1)
            go increment()
        }
        wg.Wait()
        fmt.Println("Value of sharedVar:", sharedVar)
    }
    ```

9. **Ge ett exempel p√• ett program i Go med kanaler som kan leda till deadlock**
    ```go
    package main
    func main() {
        ch := make(chan int)
        ch <- 1 // Denna operation v√§ntar p√• att n√•gon ska l√§sa fr√•n kanalen, vilket leder till deadlock eftersom det inte finns n√•gra andra goroutines.
    }
    ```

10. **Ge ett exempel p√• ett program i Go utan kanaler som kan leda till deadlock**
    ```go
    package main
    import (
        "sync"
    )
    func main() {
        var mutex1, mutex2 sync.Mutex
        go func() {
            mutex1.Lock()
            mutex2.Lock()
            mutex2.Unlock()
            mutex1.Unlock()
        }()
        mutex2.Lock()
        mutex1.Lock()
        mutex1.Unlock()
        mutex2.Unlock()
        // Deadlock kan uppst√• om den f√∂rsta goroutinen f√•r tag p√• mutex1 men sedan preempts innan den l√•ser mutex2,
        // och huvudgoroutinen lyckas l√•sa mutex2.
    }
11. **Implementera en parallel prefix sum med goroutines i Go**
    ```go
    // Detta exempel illustrerar en enkel form och kanske inte helt representerar en optimal parallel prefix sum implementation.
    package main
    import (
        "fmt"
        "sync"
    )
    func parallelPrefixSum(arr []int, result []int, start, end int, wg *sync.WaitGroup) {
        sum := 0
        for i := start; i < end; i++ {
            sum += arr[i]
            result[i] = sum
        }
        wg.Done()
    }
    func main() {
        arr := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
        result := make([]int, len(arr))
        var wg sync.WaitGroup
        parts := 2 // Antalet delar att dela upp datan i
        partSize := len(arr) / parts
        for i := 0; i < parts; i++ {
            wg.Add(1)
            start := i * partSize
            end := start + partSize
            if i == (parts - 1) {
                end = len(arr)
            }
            go parallelPrefixSum(arr, result, start, end, &wg)
        }
        wg.Wait()
        fmt.Println("Result:", result)
    }
    ```

12. **Implementera en parallel prefix scan med goroutines i Go**
    ```go
    // Detta exempel √§r liknande f√∂reg√•ende och illustrerar en grundl√§ggande tillv√§gag√•ngss√§tt.
    package main
    import (
        "fmt"
        "sync"
    )
    func parallelPrefixScan(arr []int, result []int, start, end int, wg *sync.WaitGroup) {
        if start == 0 {
            result[start] = arr[start]
            start++
        }
        for i := start; i < end; i++ {
            result[i] = result[i-1] + arr[i]
        }
        wg.Done()
    }
    func main() {
        arr := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
        result := make([]int, len(arr))
        var wg sync.WaitGroup
        parts := 2
        partSize := len(arr) / parts
        for i := 0; i < parts; i++ {
            wg.Add(1)
            start := i * partSize
            end := start + partSize
            if i == (parts - 1) {
                end = len(arr)
            }
            go parallelPrefixScan(arr, result, start, end, &wg)
        }
        wg.Wait()
        fmt.Println("Result:", result)
    }
    ```

13. **Implementera en parallel odd-even transposition sort med goroutines i Go**
    ```go
    // Denna implementation √§r f√∂renklad och kanske inte helt effektiv.
    package main
    import (
        "fmt"
        "sync"
    )
    func oddEvenTranspositionSort(arr []int) {
        n := len(arr)
        sorted := false
        for !sorted {
            sorted = true
            for i := 1; i < n-1; i += 2 {
                if arr[i] > arr[i+1] {
                    arr[i], arr[i+1] = arr[i+1], arr[i]
                    sorted = false
                }
            }
            for i := 0; i < n-1; i += 2 {
                if arr[i] > arr[i+1] {
                    arr[i], arr[i+1] = arr[i+1], arr[i]
                    sorted = false
                }
            }
        }
    }
    func main() {
        arr := []int{5, 1, 4, 2, 8, 0, 2}
        oddEvenTranspositionSort(arr)
        fmt.Println("Sorted:", arr)
    }
    ```

14. **Visa hur en for-loop kan paralleliseras i Go med goroutines**
    ```go
    package main
    import (
        "fmt"
        "sync"
    )
    func process(i int, wg *sync.WaitGroup) {
        fmt.Printf("Processing %d\n", i)
        wg.Done()
    }
    func main() {
        var wg sync.WaitGroup
        for i := 0; i < 10; i++ {
            wg.Add(1)
            go process(i, &wg)
        }
        wg.Wait()
    }
    ```

15. **Hur kan en barrier implementeras med mutex? Visa i Go-liknande kod**
    ```go
    package main
    import (
        "fmt"
        "sync"
    )
    type Barrier struct {
        total int
        count int
        mutex sync.Mutex
        cond  *sync.Cond
    }
    func NewBarrier(total int) *Barrier {
        b := &Barrier{total: total}
        b.cond = sync.NewCond(&b.mutex)
        return b
    }
    func (b *Barrier) Wait() {
        b.mutex.Lock()
        b.count++
        if b.count == b.total {
            b.count = 0 // Reset for next use
            b.cond.Broadcast()
        } else {
            b.cond.Wait()
        }
        b.mutex.Unlock()
    }
    func main() {
        var wg sync.WaitGroup
        barrier := NewBarrier(3)
        for i := 0; i < 3; i++ {
            wg.Add(1)
            go func(n int) {
                defer wg.Done()
                fmt.Printf("Goroutine %d reaching barrier\n", n)
                barrier.Wait()
                fmt.Printf("Goroutine %d passed barrier\n", n)
            }(i)
        }
        wg.Wait()
    }
    ```

16. **Hur kan en barrier implementeras med kanaler? Visa i Go-liknande kod**
    ```go
    package main
    import (
        "fmt"
        "sync"
    )
    func barrier(n int, wg *sync.WaitGroup, ch chan bool) {
        wg.Done()
        wg.Wait()
        ch <- true
    }
    func main() {
        var wg sync.WaitGroup
        ch := make(chan bool)
        wg.Add(3)
        for i := 0; i < 3; i++ {
            go func(n int) {
                fmt.Printf("Goroutine %d reaching barrier\n", n)
                barrier(3, &wg, ch)
                <-ch
                fmt.Printf("Goroutine %d passed barrier\n", n)
            }(i)
        }
        close(ch) // Ensure no deadlock
    }
    ```

17. **Ge en concurrent/parallel algoritm f√∂r att hitta det minsta v√§rdet i en lista i Go-liknande kod**
    ```go
    package main
    import (
        "fmt"
        "sync"
    )
    func findMin(nums []int, ch chan int) {
        min := nums[0]
        for _, n := range nums {
            if n < min {
                min = n
            }
        }
        ch <- min
    }
    func main() {
        nums := []int{4, 2, 6, 3, 1, 5}
        ch := make(chan int, 2)
        go findMin(nums[:len(nums)/2], ch)
        go findMin(nums[len(nums)/2:], ch)
        min1, min2 := <-ch, <-ch
        if min1 < min2 {
            fmt.Println("Min:", min1)
        } else {
            fmt.Println("Min:", min2)
        }
    }
    ```

18. **Vad inneb√§r en atom√§r variabel i Go**
    Atom√§ra variabler i Go hanteras genom paketet `sync/atomic`. En atom√§r operation utf√∂rs som en enda enhet utan m√∂jlighet f√∂r andra tr√•dar att se den i ett ofullst√§ndigt tillst√•nd, vilket s√§kerst√§ller konsistens utan att anv√§nda tunga l√•sningsmekanismer.

19. **Visa hur en variabels v√§rde kan j√§mf√∂ras och bytas atom√§rt i Go**
    ```go
    package main
    import (
        "fmt"
        "sync/atomic"
    )
    func main() {
        var value int32 = 2
        if atomic.CompareAndSwapInt32(&value, 2, 3) {
            fmt.Println("Value was 2, now 3")
        }
        fmt.Println("Current value:", value)
    }
    ```

20. **F√∂rklara vad happens-before i en minnesmodell**
    "Happens-before" √§r en relation i en minnesmodell som s√§kerst√§ller minneskonsistens genom att definiera en ordning p√• √•tkomsten till minnet. Om en √•tg√§rd A "happens-before" √•tg√§rd B, garanteras det att √§ndringar av minnet som utf√∂rs av A √§r synliga f√∂r B.

21. **Vilka antaganden kan g√∂ras om tv√• tr√•dar modiferar en delad variabel (enligt Javas minnesmodell).**
    I Javas minnesmodell, om tv√• tr√•dar modiferar en delad variabel utan synkronisering, kan ingen garanti ges om ordningen i vilken dessa modifikationer sker eller n√§r √§ndringarna blir synliga f√∂r andra tr√•dar. Detta kan leda till race conditions d√§r programmets utfall blir of√∂ruts√§gbart.

22. **F√∂rklara begreppet multicore cpu**
    En multicore CPU √§r en processor som inneh√•ller flera oberoende k√§rnor som kan l√§sa och exekvera programinstruktioner. Detta g√∂r det m√∂jligt f√∂r datorn att utf√∂ra flera processer eller tr√•dar parallellt, vilket √∂kar dess totala bearbetningskapacitet.

23. **Vad √§r skillnanden mellan concurrency och parallelism?**
    Concurrency √§r konceptet att hantera flera uppgifter samtidigt inom ett program eller system. Parallelism √§r n√§r dessa uppgifter faktiskt k√∂rs samtidigt p√• olika processork√§rnor. Concurrency handlar om struktur, medan parallelism handlar om utf√∂rande.

24. **Vad betyder det att ett problem √§r embarrassingly parallel?**
    Ett problem √§r "embarrassingly parallel" om det kan delas upp i m√•nga oberoende deluppgifter som kan utf√∂ras samtidigt med liten till ingen behov f√∂r kommunikation eller beroenden mellan uppgifterna.

25. **Ge exempel p√• ett problem som √§r embarrassingly parallel**
    Exempel p√• embarrassingly parallel problem inkluderar bildbehandling d√§r varje pixel eller bildblock kan bearbetas oberoende, eller stora simuleringar d√§r ber√§kningar f√∂r varje delsystem √§r oberoende av andra delsystem.

26. **F√∂rklara begreppet operativsystem**
    Ett operativsystem (OS) √§r programvara som hanterar datorns h√•rdvara och programresurser, och tillhandah√•ller gemensamma tj√§nster f√∂r datorprogram. Det fungerar som en mellanhand mellan anv√§ndarna av en dator och datorns h√•rdvara.

27. **Vad betyder det att ett operativsystem kan ses som ett interface mot datorn?**
    Att ett operativsystem kan ses som ett interface mot datorn betyder att det tillhandah√•ller en anv√§ndarv√§nlig gr√§nssnitt f√∂r m√§nniskor att interagera med datorns h√•rdvara, g√∂mmer komplexiteten i h√•rdvaruhanteringen och till√•ter k√∂rning och styrning av applikationer.

28. **Vad betyder det att ett operativsystem kan ses som ett kontrollsystem f√∂r datorn?**
    Det inneb√§r att operativsystemet √∂vervakar och styr tillg√•ngen till h√•rdvaruresurser som CPU, minne och I/O-enheter. Det allokerar resurser till olika processer och program, och ser till att systemet fungerar effektivt och r√§ttvist.

29. **Vad √§r skillnaden p√• system mode och user mode?**
    Skillnaden mellan system mode (√§ven kallad kernel mode) och user mode √§r att i system mode har koden full tillg√•ng till alla h√•rdvaruresurser och kan k√∂ra privilegierade instruktioner. I user mode √§r koden begr√§nsad f√∂r att skydda systemet fr√•n potentiellt skadliga eller felaktiga operationer.

30. **Ange fem tillst√•nd en process kan befinna sig i och f√∂rklara vad de inneb√§r**
    - **Ny (New):** Processen har skapats men har √§nnu inte tilldelats de resurser som kr√§vs f√∂r att k√∂ra.
    - **K√∂r (Running):** Processen utf√∂r sina instruktioner p√• processorn.
    - **V√§ntande (Waiting):** Processen v√§ntar p√• n√•gon h√§ndelse, till exempel I/O-operationer eller signaler fr√•n andra processer.
    - **Redo (Ready):** Processen √§r redo att k√∂ra och v√§ntar p√• att tilldelas processorn.
    - **Avslutad (Terminated):** Processen har avslutats och v√§ntar p√• att dess resurser ska frig√∂ras av operativsystemet.
31. **F√∂rklara begreppen stack, heap, data och text i relation till processer**
    - **Stack:** Ett omr√•de av minnet som anv√§nds f√∂r att lagra lokala variabler och kontrollinformation f√∂r varje tr√•d eller funktion. Stacken v√§xer och krymper dynamiskt n√§r funktioner anropas och √•terv√§nder.
    - **Heap:** Ett omr√•de av minnet som anv√§nds f√∂r dynamisk minnesallokering under programk√∂rning. Till skillnad fr√•n stacken, hanteras minnet p√• heapen explicit av utvecklaren (tilldela och frig√∂ra).
    - **Data:** Segmentet inneh√•ller globala och statiska variabler som √§r initialiserade av programmet.
    - **Text:** √Ñven kallad kodsegmentet, inneh√•ller detta de faktiska maskininstruktionerna som ska utf√∂ras av processorn. Det √§r ofta skrivskyddat f√∂r att f√∂rhindra att programmet av misstag modifierar sin egen kod.

32. **Vad lagras i ett processkontrollblock (PCB)?**
    Ett Processkontrollblock (PCB) lagrar viktig information om processens tillst√•nd, inklusive processidentifierare (PID), programr√§knare, registerstatus, minnesallokeringsinformation, √∂ppna filhanterare, s√§kerhetsattribut, processprioritet och pekare till schemal√§ggningsk√∂er.

33. **Vad √§r en context switch?**
    En context switch √§r processen d√§r operativsystemet sparar tillst√•ndet f√∂r en k√∂rande process eller tr√•d s√• att den senare kan √•terupptas. Detta g√∂r det m√∂jligt f√∂r CPU:n att v√§xla mellan processer/tr√•dar och utnyttja dess kapacitet effektivt genom multitasking.

34. **F√∂rklara vad som h√§nder under en context switch**
    Under en context switch sparar operativsystemet tillst√•ndet f√∂r den nuvarande processen/tr√•den till dess PCB och laddar tillst√•ndet f√∂r den n√§sta processen/tr√•den att k√∂ras fr√•n dess PCB. Detta innefattar att byta ut registerinneh√•ll, programr√§knare och minneskartl√§ggning, vilket g√∂r att flera processer/tr√•dar kan dela p√• samma CPU utan att st√∂ra varandra.

35. **Vad inneb√§r schemal√§ggning av en process?**
    Schemal√§ggning av en process inneb√§r att besluta vilken process eller tr√•d som ska tilldelas CPU:n att k√∂ras vid en given tidpunkt. Schemal√§ggaren anv√§nder olika algoritmer f√∂r att optimera prestanda, responsivitet och r√§ttvisa bland processerna.

36. **N√§r tas beslut om schemal√§ggning?**
    Beslut om schemal√§ggning tas vid specifika h√§ndelser som n√§r en process blir redo att k√∂ras, n√§r en process avslutas eller blockerar f√∂r I/O, och vid avbrott fr√•n timern som indikerar att den nuvarande processens tilldelade tid har g√•tt ut.

37. **Vad g√∂r dispatcher?**
    Dispatchern √§r en komponent av operativsystemets schemal√§ggare som ansvarar f√∂r att genomf√∂ra beslutet som tagits av schemal√§ggaren. Det inneb√§r att byta kontext till den valda processen, vilket inneb√§r att ladda dess tillst√•nd s√• att den kan b√∂rja eller √•teruppta k√∂rning p√• CPU:n.

38. **Ange tre kriterier som kan anv√§ndas f√∂r att best√§mma vilken schemal√§ggningsalgoritm som ska anv√§ndas**
    - **Responsivitet:** Viktigt f√∂r interaktiva system d√§r anv√§ndaren f√∂rv√§ntar sig snabba svar.
    - **R√§ttvisa:** Alla processer f√•r r√§ttvis tillg√•ng till CPU-resurser.
    - **Genomstr√∂mning:** Maximera antalet processer som slutf√∂rs per tidsenhet.
39. **Vad betyder det att en process √§r I/O bound?**
    En process √§r I/O bound om dess utf√∂rande √§r begr√§nsat av I/O-operationer, vilket betyder att den spenderar mer tid p√• att v√§nta p√• I/O √§n att utf√∂ra ber√§kningar. S√•dana processer gynnas av snabbare I/O-system snarare √§n snabbare CPU.

40. **Vad betyder det att en process √§r CPU bound?**
    En process √§r CPU bound om dess utf√∂rande huvudsakligen begr√§nsas av processorns hastighet. Dessa processer spenderar merparten av sin tid p√• att utf√∂ra ber√§kningar och gynnas av en snabbare CPU.

41. **F√∂rklara hur schemal√§ggning sker om First Come, First Served anv√§nds**
    Med First Come, First Served (FCFS) schemal√§ggningsalgoritm, hanteras processerna i den ordning de anl√§nder till k√∂n. Den f√∂rsta processen i k√∂n tilldelas CPU:n och k√∂r till avslut utan preemption. Detta √§r enkelt att implementera men kan leda till l√•ng v√§ntetid f√∂r processer bakom en tidskr√§vande process.

42. **F√∂rklara hur schemal√§ggning sker om Round Robin anv√§nds**
    Round Robin-algoritmen tilldelar varje process en liten tidsskiva (kvantum) och k√∂r dem i tur och ordning. Om en process inte avslutas under sin tidsskiva, placeras den i slutet av k√∂n. Detta forts√§tter i en loop, vilket ger alla processer en r√§ttvis CPU-tid och reducerar v√§ntetiden f√∂r interaktiva processer.

43. **F√∂rklara preemptive schemal√§ggning**
    Preemptive schemal√§ggning inneb√§r att operativsystemet kan avbryta en k√∂rande process och √•terf√∂ra den till redo-k√∂n f√∂r att ge plats √•t en annan process. Detta anv√§nds f√∂r att f√∂rb√§ttra systemets responsivitet och f√∂r att s√§kerst√§lla att h√∂gprioriterade processer f√•r CPU-tid n√§r de beh√∂ver det.

44. **F√∂rklara non-preemptive schemal√§ggning**
    I non-preemptive schemal√§ggning, n√§r en process v√§l b√∂rjat k√∂ra, forts√§tter den att k√∂ra tills den blockerar (t.ex. f√∂r I/O) eller avslutas. Den nuvarande processen kan inte avbrytas av schemal√§ggaren till f√∂rm√•n f√∂r en annan process, vilket kan leda till mindre effektiv CPU-anv√§ndning j√§mf√∂rt med preemptive schemal√§ggning.

45. **Vad √§r ett quantum?**
    Quantum, √§ven kallat tidsskiva, √§r den tid en process till√•ts k√∂ra i en preemptive multitasking milj√∂, som i Round Robin-schemal√§ggning. Efter att en process har k√∂rt f√∂r en kvantumtid, avbryts den s√• att n√§sta process i k√∂n kan k√∂ra.

46. **Hur p√•verkar ett quantum schemal√§ggningen?**
    Storleken p√• kvantumet p√•verkar direkt systemets responsivitet och genomstr√∂mning. Ett kort quantum f√∂rb√§ttrar responsiviteten genom att till√•ta processer att v√§xla ofta, men kan √∂ka overhead f√∂r context switching. Ett l√•ngt quantum minskar overhead men kan g√∂ra systemet mindre responsivt.

47. **Vad √§r viktigt att t√§nka p√• n√§r man best√§mmer quantum?**
    Vid best√§mning av quantumstorleken √§r det viktigt att balansera systemets responsivitet mot overhead f√∂r context switching. Quantumstorleken b√∂r vara tillr√§ckligt l√•ng f√∂r att utf√∂ra meningsfullt arbete men tillr√§ckligt kort f√∂r att h√•lla systemet responsivt, s√§rskilt i en milj√∂ med m√•nga interaktiva processer.

48. **F√∂rklara begreppen isolation och encapsulation i relation till processer**
    Isolation inneb√§r att varje process k√∂rs i sitt eget skyddade adressutrymme, vilket f√∂rhindrar att processer st√∂r eller korrumperar varandras data. Encapsulation refererar till att en process endast kan √•tkomma sina egna resurser och data, skyddade av operativsystemet, vilket fr√§mjar s√§kerhet och stabilitet i systemet.

49. **Vad √§r skillnaden p√• en process och en tr√•d?**
    En process √§r en oberoende enhet som inneh√•ller ett program under k√∂rning, inklusive kod, data och systemresurser. En tr√•d √§r en l√§ttviktig process som delar processens resurser men kan k√∂ras parallellt inom samma process. Tr√•dar har sitt eget stackutrymme men delar kod, data och andra resurser.

50. **Vad anv√§nds stacken till i en tr√•d?**
    Stacken i en tr√•d anv√§nds f√∂r att lagra lokala variabler, parametrar och returadresser f√∂r funktionanrop. Varje tr√•d har sin egen stack som g√∂r det m√∂jligt att h√•lla koll p√• dess individuella utf√∂rande historia och tillst√•nd.
51. **Var √§r skillnaden p√• user-level- och kernel-level-tr√•dar?**
    - **User-level tr√•dar:** Dessa tr√•dar hanteras och schemal√§ggs helt av anv√§ndarprocessen utan direkt inblandning av operativsystemets k√§rna. De √§r snabbare att skapa och v√§xla mellan √§n kernel-tr√•dar, men en blockerad tr√•d kan blockera hela processen eftersom k√§rnan inte √§r medveten om de individuella tr√•darna.
    - **Kernel-level tr√•dar:** Dessa tr√•dar hanteras direkt av operativsystemets k√§rna. Detta g√∂r dem l√•ngsammare att skapa och v√§xla mellan, men en blockerad tr√•d p√•verkar inte andra tr√•dar i samma process. Dessutom kan k√§rnan schemal√§gga tr√•dar fr√•n olika processer parallellt p√• multiprocessorsystem.

52. **F√∂rklara begreppet blockerande anrop**
    Blockerande anrop inneb√§r att ett anrop till en funktion eller systemkallelse pausar utf√∂randet av den anropande tr√•den eller processen tills operationen √§r slutf√∂rd. Under denna tid kan tr√•den eller processen inte utf√∂ra n√•got annat arbete, vilket kan leda till ineffektivitet om resursen den v√§ntar p√• √§r l√•ngsam.

53. **Vad h√§nder om en user-level tr√•d g√∂r ett blockerande anrop?**
    Om en user-level tr√•d g√∂r ett blockerande anrop, kommer hela processen att blockeras och v√§nta p√• att anropet slutf√∂rs. Detta beror p√• att operativsystemets k√§rna inte kan skilja p√• de enskilda user-level tr√•darna och ser hela processen som en enhet.

54. **Vad h√§nder om en kernel-level tr√•d g√∂r ett blockerande anrop?**
    Om en kernel-level tr√•d g√∂r ett blockerande anrop, p√•verkas bara den specifika tr√•den. Andra tr√•dar inom samma process eller i andra processer kan forts√§tta att k√∂ras obehindrat eftersom operativsystemets k√§rna kan hantera och schemal√§gga varje kernel-level tr√•d individuellt.

55. **Varf√∂r vill man dela minne mellan tr√•dar?**
    Att dela minne mellan tr√•dar m√∂jligg√∂r effektiv kommunikation och datautbyte utan behov av dyra interprocesskommunikationsmekanismer. Det g√∂r det ocks√• m√∂jligt f√∂r tr√•dar att samarbeta om att utf√∂ra uppgifter genom att manipulera gemensamma datastrukturer och resurser.

56. **Vad √§r skillnaden mellan att dela minne mellan tr√•dar och processer?**
    Att dela minne mellan tr√•dar inneb√§r att tr√•darna inom samma process kan √•tkomma och modifiera samma minnesomr√•den direkt eftersom de delar processens adressutrymme. Delning av minne mellan processer kr√§ver speciella interprocesskommunikationsmekanismer (som delat minne, meddelandek√∂er) eftersom varje process har ett eget isolerat adressutrymme.

57. **F√∂rklara begreppet sequentially consistent i relation till minne**
    Sequentially consistent minne inneb√§r att resultatet av alla minnesoperationer ser ut som om de hade utf√∂rts i en strikt sekventiell ordning, √§ven om de i verkligheten kan ha utf√∂rts parallellt eller i olika ordningar p√• olika processorer. Det garanterar att alla tr√•dar ser alla minnesoperationer i samma ordning.

58. **Vad inneb√§r ett data race/race condition?**
    Ett data race eller race condition uppst√•r n√§r tv√• eller fler tr√•dar eller processer f√∂rs√∂ker √§ndra eller l√§sa en delad resurs samtidigt utan l√§mplig synkronisering, vilket leder till of√∂ruts√§gbart beteende eller felaktiga resultat beroende p√• exekveringsordningen.

59. **Vad √§r en kritisk sektion?**
    En kritisk sektion √§r en del av koden som √•tkommer en delad resurs som inte f√•r modifieras samtidigt av flera tr√•dar. F√∂r att f√∂rhindra data races, m√•ste √•tkomst till kritiska sektioner synkroniseras s√• att endast en tr√•d √•t g√•ngen kan exekvera den kritiska sektionen.

60. **Ge exempel p√• ett data race**
    Ett exempel p√• ett data race √§r tv√• tr√•dar som samtidigt f√∂rs√∂ker uppdatera en global r√§knare utan synkronisering. Om b√•da tr√•darna l√§ser r√§knarens v√§rde, inkrementerar det, och sedan skriver tillbaka det n√§stan samtidigt, kan en av uppdateringarna skrivas √∂ver, vilket resulterar i att r√§knaren endast √∂kas med 1 ist√§llet f√∂r 2.
61. **Vad inneb√§r √∂mesidig uteslutning?**
    √ñmsesidig uteslutning √§r en princip som s√§kerst√§ller att n√§r en tr√•d utf√∂r en kritisk sektion av koden, inga andra tr√•dar kan exekvera samma eller n√•gon annan kritisk sektion som manipulerar samma delade resurser. Detta f√∂rhindrar data races och garanterar korrekthet i √•tkomsten till delade data.

62. **Hur kan √∂mesidig uteslutning l√∂sas? Ge exempel.**
    √ñmsesidig uteslutning kan uppn√•s genom anv√§ndning av olika synkroniseringsmekanismer, s√•som:
    - **L√•s (Mutex):** Ger exklusiv √•tkomst till en resurs f√∂r en tr√•d √•t g√•ngen.
    - **Semaphores:** En mer generell mekanism som kan begr√§nsa antalet tr√•dar som √•tkommer en resurs.
    - **Monitorer:** Ett h√∂gniv√•koncept som inkapslar synkroniserade metoder eller operationer p√• en resurs.

    ```go
    var mutex sync.Mutex
    func criticalSection() {
        mutex.Lock()
        // Kritisk sektion: modifiera delade resurser h√§r
        mutex.Unlock()
    }
    ```

63. **F√∂rklara hur strict alteration f√∂rs√∂ker l√∂sa √∂mesidig uteslutning. Vilka problem finns?**
    Strict alteration √§r en teknik f√∂r att uppn√• √∂msesidig uteslutning d√§r tv√• processer alternerar strikt mellan att ha till√•telse att k√∂ra sina kritiska sektioner. Detta implementeras ofta med en delad variabel som h√•ller koll p√• vem som har tur att exekvera. Problemet med denna metod √§r att den kan leda till on√∂dig v√§ntan (busy waiting) och inte √§r skalbar till flera processer eller tr√•dar.

64. **F√∂rklara Peterson‚Äôs algoritm f√∂r √∂msesidig uteslutning**
    Petersons algoritm √§r en l√∂sning p√• problemet med √∂msesidig uteslutning f√∂r tv√• processer/tr√•dar. Den anv√§nder tv√• flaggor (en f√∂r varje process/tr√•d) och en turn-variabel f√∂r att avg√∂ra vilken process som f√•r tilltr√§de till sin kritiska sektion. Varje process s√§tter sin flagga till `true` f√∂r att indikera att den vill k√∂ra sin kritiska sektion och s√§tter `turn` till den andra processens ID. Processen f√•r k√∂ra sin kritiska sektion endast om den andra processens flagga √§r `false` eller om det √§r dess tur (baserat p√• `turn`-variabeln). Efter att ha k√∂rt sin kritiska sektion, s√§tter processen sin flagga till `false`.

65. **Vad inneb√§r progression i relation till √∂mesidig uteslutning?**
    Progression inom √∂msesidig uteslutning inneb√§r att om en process eller tr√•d beg√§r tilltr√§de till sin kritiska sektion, kommer systemet s√• sm√•ningom att till√•ta den att forts√§tta. Det garanterar att beg√§randen om √•tkomst till kritiska sektioner inte ignoreras f√∂r alltid, vilket f√∂rhindrar l√•sningar (deadlocks) och hunger (starvation).

66. **Vad inneb√§r begr√§nsad v√§ntan i relation till √∂mesidig uteslutning?**
    Begr√§nsad v√§ntan √§r ett krav i algoritmer f√∂r √∂msesidig uteslutning som s√§kerst√§ller att det finns en √∂vre gr√§ns p√• antalet g√•nger andra processer f√•r tilltr√§de till sina kritiska sektioner efter att en process har beg√§rt tilltr√§de till sin kritiska sektion och innan beg√§ran beviljas. Detta f√∂rhindrar att en process v√§ntar o√§ndligt l√§nge p√• att f√• k√∂ra sin kritiska sektion.

67. **Vad √§r en semafor?**
    En semafor √§r en synkroniseringsmekanism som anv√§nds f√∂r att kontrollera √•tkomst till en gemensam resurs genom flera processer i ett konkurrent system eller av flera tr√•dar i en multiprogrammerad milj√∂. Semaforer kan vara bin√§ra (liknar mutex) eller kunna r√§kna, vilket till√•ter ett visst antal tr√•dar att √•tkomma en resurs samtidigt.

68. **Vad √§r ett mutex lock?**
    Ett mutex lock (mutual exclusion lock) √§r en synkroniseringsmekanism som anv√§nds f√∂r att s√§kerst√§lla √∂msesidig uteslutning n√§r flera tr√•dar f√∂rs√∂ker √•tkomma samma resurser. Ett mutex till√•ter endast en tr√•d att √•tkomma den kritiska sektionen √•t g√•ngen, vilket f√∂rhindrar race conditions.

69. **Vad √§r skillnaden mellan en bin√§r och en r√§knande semafor?**
    En bin√§r semafor, eller mutex, √§r en typ av semafor som endast kan ha tv√• tillst√•nd: l√•st eller ol√•st. Den anv√§nds f√∂r √∂msesidig uteslutning. En r√§knande semafor till√•ter ett visst antal tr√•dar att √•tkomma en resurs samtidigt, d√§r "r√§knaren" anger det maximala antalet tr√•dar som kan ha samtidig √•tkomst.
70. **Visa hur en r√§knande semafor kan implementeras med hj√§lp av bin√§ra semaforer**
    Implementeringen av en r√§knande semafor med bin√§ra semaforer (mutexes) kr√§ver en kombination av en mutex f√∂r att skydda tillg√•ngen till r√§knaren och en mekanism f√∂r att blockera och v√§cka tr√•dar baserat p√• r√§knarens v√§rde. H√§r √§r en konceptuell beskrivning snarare √§n specifik kod:

    ```pseudo
    Initialize:
    count = N // N √§r antalet till√•tna tr√•dar att √•tkomma resursen samtidigt
    mutex = Semaphore(1) // Bin√§r semafor/mutex f√∂r att skydda r√§knaren
    blockQueue = Queue() // K√∂ f√∂r tr√•dar som v√§ntar p√• √•tkomst

    wait():
    mutex.acquire()
    if count > 0:
        count -= 1
    else:
        blockQueue.enqueue(thread)
        mutex.release()
        block() // Blockera den h√§r tr√•den
    mutex.release()

    signal():
    mutex.acquire()
    if blockQueue not empty:
        thread = blockQueue.dequeue()
        wakeup(thread) // V√§ck upp tr√•den
    else:
        count += 1
    mutex.release()
    ```
    Denna pseudo-kod visar grundprincipen f√∂r hur en r√§knande semafor kan simuleras med en bin√§r semafor. `wait()`-metoden minskar r√§knaren om det finns tillg√§ngliga "platser" och blockerar tr√•den om inte. `signal()`-metoden v√§cker en blockerad tr√•d om s√•dan finns, eller √∂kar r√§knaren om inte.

71. **Vilka risker finns det med l√•sning?**
    L√•sning medf√∂r flera risker, inklusive:
    - **Deadlocks:** Kan uppst√• n√§r tv√• eller fler processer v√§ntar p√• varandra f√∂r att frig√∂ra l√•s, vilket skapar en cirkul√§r beroendekedja d√§r ingen kan forts√§tta.
    - **Starvation:** N√§r en eller flera tr√•dar inte f√•r tillg√•ng till en resurs f√∂r en orimligt l√•ng tid p√• grund av andra tr√•dars l√•ngvariga eller upprepade √•tkomst.
    - **Priority Inversion:** N√§r en h√∂gre prioriterad tr√•d v√§ntar p√• en resurs som h√•lls av en l√§gre prioriterad tr√•d, vilket leder till att systemets prestanda blir s√§mre √§n f√∂rv√§ntat.
    - **√ñverdriven l√•sning:** Kan leda till minskad parallellitet och s√§mre systemprestanda.

72. **Vad √§r ett deadlock?**
    Ett deadlock uppst√•r n√§r tv√• eller flera processer/tr√•dar v√§ntar p√• varandra f√∂r att frig√∂ra resurser eller l√•s, vilket skapar en situation d√§r ingen av dem kan forts√§tta. Varje process i deadlocksituationen v√§ntar p√• en resurs som h√•lls av en annan process i samma upps√§ttning.

73. **Vad inneb√§r det att en process √§r deadlocked?**
    N√§r en process √§r deadlocked, √§r den permanent blockerad och kan inte forts√§tta sin exekvering eftersom den v√§ntar p√• en resurs eller ett l√•s som aldrig kommer att frig√∂ras av de andra processerna/tr√•darna inblandade i deadlocksituationen.

74. **Vilka fyra villkor kr√§vs f√∂r deadlock?**
    De fyra villkoren som m√•ste vara uppfyllda f√∂r att en deadlock ska intr√§ffa √§r:
    - **√ñmsesidig uteslutning:** Minst en resurs m√•ste vara i ett icke-delbart l√§ge, d√§r bara en process kan anv√§nda resursen √•t g√•ngen.
    - **H√•ll och v√§nta:** Processer h√•ller minst en resurs och v√§ntar p√• att f√• ytterligare resurser som h√•lls av andra processer.
    - **Ingen f√∂rdrivning (No Preemption):** Resurser kan inte tas fr√•n processer; de m√•ste frig√∂ras frivilligt.
    - **Cirkul√§r v√§ntan:** Det finns en upps√§ttning av processer {P1, P2, ..., Pn} d√§r varje process Pi v√§ntar p√• en resurs som h√•lls av Pi+1 (och Pn v√§ntar p√• P1), vilket skapar en sluten kedja av beroenden.
75. **F√∂rklara hold and wait**
    "Hold and wait" √§r ett av de villkor som kan leda till deadlock. Det uppst√•r n√§r processer h√•ller p√• en eller flera resurser samtidigt som de v√§ntar p√• att f√• ytterligare resurser som √§r upptagna av andra processer. Detta villkor bidrar till potentiella deadlocks eftersom det skapar en situation d√§r processer kan blockera varandra.

76. **F√∂rklara no preemption**
    "No preemption" refererar till principen att en resurs inte kan tas bort fr√•n en process f√∂rr√§n processen frivilligt frig√∂r resursen. Detta villkor √§r n√∂dv√§ndigt f√∂r ett deadlock eftersom om systemet kunde f√∂rdriva resurser, skulle det kunna bryta deadlocks genom att tvinga bort resurser fr√•n blockerade processer.

77. **F√∂rklara circular wait**
    "Circular wait" uppst√•r n√§r det finns en st√§ngd kedja av processer d√§r varje process h√•ller p√• minst en resurs som n√§sta process i kedjan beh√∂ver. Den sista processen i denna kedja v√§ntar p√• en resurs som h√•lls av den f√∂rsta processen, vilket skapar en on√∂digt cirkul√§r beroendekedja d√§r inga processer kan forts√§tta.

78. **F√∂rklara hur deadlock kan f√∂rhindras**
    Deadlock kan f√∂rhindras genom att bryta minst ett av de fyra villkoren som √§r n√∂dv√§ndiga f√∂r att ett deadlock ska intr√§ffa:
    - **Eliminera √∂msesidig uteslutning:** G√∂r det m√∂jligt f√∂r flera processer att dela vissa resurser, om m√∂jligt.
    - **Undvik hold and wait:** Tilldela alla n√∂dv√§ndiga resurser till en process innan exekvering b√∂rjar eller kr√§v att processer frig√∂r alla resurser innan de beg√§r nya.
    - **Till√•t preemption:** G√∂r det m√∂jligt att f√∂rdriva resurser fr√•n processer om de resurserna beh√∂vs f√∂r att undvika deadlock.
    - **Undvik circular wait:** Inf√∂r en ordning p√• hur resurser beg√§rs f√∂r att f√∂rhindra st√§ngda kedjor av beroenden.

79. **Hur kan mutual exclusion f√∂rhindras f√∂r att undvika deadlock?**
    F√∂r att f√∂rhindra mutual exclusion och d√§rmed minska risken f√∂r deadlock, kan systemet f√∂rs√∂ka minimera antalet resurser som kr√§ver √∂msesidig uteslutning eller anv√§nda algoritmer som till√•ter delning av resurser d√§r det √§r m√∂jligt utan att √§ventyra integritet eller prestanda.

80. **Hur kan hold and wait f√∂rhindras f√∂r att undvika deadlock?**
    F√∂r att f√∂rhindra hold and wait kan systemet kr√§va att en process beg√§r alla de resurser den beh√∂ver samtidigt och blockerar processen tills alla resurser kan tilldelas samtidigt. Alternativt kan systemet kr√§va att processer frig√∂r alla innehavda resurser innan de beg√§r nya.

81. **Hur kan no preemption f√∂rhindras f√∂r att undvika deadlock?**
    F√∂r att f√∂rhindra no preemption, kan systemet till√•ta att resurser f√∂rdrivs fr√•n en process under vissa omst√§ndigheter. Detta kan inneb√§ra att implementera en policy d√§r processer med l√§gre prioritet tvingas ge upp resurser till f√∂rm√•n f√∂r processer med h√∂gre prioritet.

82. **Hur kan circular wait f√∂rhindras f√∂r att undvika deadlock?**
    Circular wait kan f√∂rhindras genom att inf√∂ra en global ordning p√• alla resurser och kr√§va att varje process beg√§r resurser enligt denna ordning. Genom att f√∂lja en fastst√§lld ordning f√∂r resursallokering, kan systemet undvika de cirkul√§ra beroendekedjor som leder till deadlock.
83. **Ge exempel p√• hur en semafor kan anv√§ndas s√• att hold and wait inte g√§ller**
    Ett s√§tt att anv√§nda semaforer f√∂r att undvika hold and wait-tillst√•ndet √§r genom att inf√∂ra en schemal√§ggare som kontrollerar resursallokeringen. Processerna m√•ste beg√§ra alla n√∂dv√§ndiga resurser p√• en g√•ng fr√•n schemal√§ggaren. Schemal√§ggaren anv√§nder en semafor f√∂r att kontrollera √•tkomsten till resurserna och tilldelar dem endast n√§r alla beg√§rda resurser kan tilldelas samtidigt. Om inte alla resurser kan tilldelas, frig√∂rs de resurser som redan tilldelats s√• att ingen process h√•ller p√• resurser samtidigt som den v√§ntar p√• ytterligare resurser.

    ```pseudo
    semaphore scheduler = 1

    request_resources():
        wait(scheduler)
        // Beg√§r alla n√∂dv√§ndiga resurser
        if all resources available:
            allocate resources
        else:
            release any allocated resources
            place process in wait queue
        signal(scheduler)

    release_resources():
        wait(scheduler)
        // Frig√∂r alla innehavda resurser
        signal(scheduler)
        // F√∂rs√∂k v√§cka processer i v√§ntek√∂n om resurser nu √§r tillg√§ngliga
    ```

84. **Varf√∂r kan det vara problematiskt att f√∂rhindra no preemption**
    Att f√∂rhindra no preemption kan vara problematiskt eftersom det kr√§ver att systemet kan avbryta processers arbete och ta tillbaka resurserna de anv√§nder. Detta kan vara tekniskt sv√•rt att implementera p√• ett s√§kert s√§tt, s√§rskilt f√∂r resurser som inte enkelt kan sparas och √•terst√§llas (t.ex. n√§tverksanslutningar eller realtidsber√§kningar). Dessutom kan det leda till prestandaf√∂rluster och √∂kad komplexitet i systemdesignen.

85. **Varf√∂r kan det vara problematiskt att f√∂rhindra hold and wait**
    Att helt f√∂rhindra hold and wait kan vara problematiskt eftersom det kan kr√§va att processer m√•ste k√§nna till alla resurser de kommer att beh√∂va i f√∂rv√§g innan de startar, vilket inte alltid √§r praktiskt eller m√∂jligt. Detta kan ocks√• leda till ineffektiv resursanv√§ndning eftersom resurser m√•ste allokeras i stora block, √§ven om de inte alla anv√§nds samtidigt.

86. **F√∂rklara starvation. Varf√∂r √§r det ett problem?**
    Starvation, eller sv√§lt, uppst√•r n√§r en eller flera processer inte f√•r tillg√•ng till de resurser de beh√∂ver f√∂r att forts√§tta sitt arbete under en orimligt l√•ng tid. Detta blir ett problem eftersom det kan leda till oacceptabla prestanda f√∂r vissa processer, speciellt i system d√§r vissa processer kan dominera resurstillg√•ngen p√• bekostnad av andra. Starvation undergr√§ver systemets r√§ttvisa och kan orsaka att viktiga processer aldrig slutf√∂rs.

87. **Vad √§r en resource allocation graph och hur anv√§nds den i samband med deadlock?**
    En resource allocation graph √§r en grafisk representation av vilka processer som h√•ller p√• vilka resurser och vilka processer som v√§ntar p√• resurser. Noder representerar processer och resurser, och kanter visar relationerna mellan dem (till exempel, en kant fr√•n en process till en resurs indikerar att processen h√•ller resursen, och en kant fr√•n en resurs till en process indikerar att processen v√§ntar p√• resursen). Denna graf kan anv√§ndas f√∂r att uppt√§cka potentiella deadlocks genom att leta efter cykler; en cykel indikerar en upps√§ttning processer och resurser som √§r involverade i en deadlock.

88. **Hur kan man avg√∂ra om ett system √§r i deadlock med hj√§lp av en resource allocation graph?**
    Ett system anses vara i deadlock om dess resource allocation graph inneh√•ller minst en cykel. En cykel i grafen inneb√§r att det finns en upps√§ttning processer som v√§ntar p√• varandra i en sluten kedja, vilket √§r en direkt indikation p√• en deadlock-situation. Genom att analysera grafen f√∂r att identifiera s√•dana cykler kan systemadministrat√∂rer eller algoritmer uppt√§cka och √•tg√§rda deadlocks.

89. **√Ñr det ok att ignorera deadlock?**
    Det √§r generellt inte ok att ignorera deadlock i de flesta produktionssystem eftersom det kan leda till allvarliga prestandaproblem eller till och med fullst√§ndig systemstopp. Att ignorera deadlock kan vara acceptabelt i mycket s√§rskilda fall d√§r deadlocks √§r extremt s√§llsynta, konsekvenserna √§r minimala, eller systemet √§r utformat f√∂r att hantera s√•dana situationer p√• annat s√§tt (t.ex. genom omstart av tj√§nster).

90. **Vad √§r skillnaden p√• att f√∂rhindra och undvika deadlock?**
    Att f√∂rhindra deadlock inneb√§r att designa systemet och dess algoritmer p√• ett s√•dant s√§tt att de fyra villkoren f√∂r deadlock aldrig kan uppfyllas. √Ö andra sidan, inneb√§r att undvika deadlock att l√•ta systemet och processerna n√• tillst√•nd d√§r de potentiellt kan orsaka deadlock, men anv√§nda dynamisk resursallokering och processstyrning f√∂r att s√§kerst√§lla att systemet aldrig faktiskt g√•r in i en deadlock. Undvikande kr√§ver ofta mer sofistikerad logik och √∂vervakning av systemets tillst√•nd.

91. **F√∂rklara Banker‚Äôs algoritm**
    Banker's algoritm √§r en resursallokerings- och deadlockundvikande algoritm som anv√§nds i operativsystem. Den liknar en bankir som l√•nar ut pengar. Algoritmen h√•ller koll p√• tillg√§ngliga, allokerade, och maximala resurser kr√§vda av processer f√∂r att s√§kerst√§lla att systemet alltid √§r i ett s√§kert tillst√•nd d√§r det kan undvika deadlocks. F√∂r varje resursbeg√§ran kontrollerar algoritmen om att tilldela resurserna fortfarande skulle l√§mna systemet i ett s√§kert tillst√•nd. Om ja, till√•ts beg√§ran; om inte, v√§ntar processen f√∂r att undvika en potentiell deadlock.

92. **Hur kan ett system √•terh√§mta sig fr√•n deadlock?**
    System kan √•terh√§mta sig fr√•n deadlocks genom:
    - **Processavslutning:** Avbryta en eller flera processer som √§r involverade i deadlocken.
    - **Resursf√∂rdrivning:** Selektivt ta bort resurser fr√•n blockerade processer och tilldela dem till andra f√∂r att bryta deadlocken.
    Dessa metoder kan anv√§nda olika kriterier, s√•som prioritet, resursanv√§ndning, och v√§ntetid f√∂r att best√§mma vilka processer som ska avbrytas eller vilka resurser som ska f√∂rdrivas.

93. **F√∂rklara prefix sum**
    Prefix sum √§r en operation som f√∂r varje element i en sekvens genererar en summa av alla element upp till det elementet. Exempelvis, f√∂r sekvensen [3, 1, 4, 1, 5], √§r prefixsummorna [3, 4, 8, 9, 14]. Detta kan anv√§ndas i flera algoritmiska applikationer, inklusive parallella algoritmer f√∂r att effektivisera ber√§kningar.

94. **F√∂rklara prefix scan**
    Prefix scan √§r en generalisering av prefix sum d√§r operationen inte n√∂dv√§ndigtvis √§r en summa utan kan vara en annan associativ operation, s√•som minimum, maximum, eller multiplikation. Detta m√∂jligg√∂r ber√§kning av en sekvens d√§r varje element representerar resultatet av operationen applicerad p√• alla f√∂reg√•ende element och sig sj√§lvt.

95. **F√∂rklara odd-even transposition sort**
    Odd-even transposition sort √§r en enkel parallell sorteringsalgoritm som liknar bubbel sort. Den fungerar genom att upprepade g√•nger genomf√∂ra tv√• pass √∂ver elementen: ett som j√§mf√∂r och eventuellt byter plats p√• element p√• udda positioner med deras n√§sta grannar (dvs. elementen p√• position 1 med 2, 3 med 4, etc.) och ett som g√∂r detsamma f√∂r element p√• j√§mna positioner. Denna process upprepas tills listan √§r sorterad.

96. **Varf√∂r √§r det en d√•lig id√© att skapa nya tr√•dar f√∂r varje rekursivt anrop i en divide and conquer-algoritm?**
    Att skapa en ny tr√•d f√∂r varje rekursivt anrop i en divide and conquer-algoritm kan snabbt leda till √∂verdriven anv√§ndning av systemresurser, som tr√•dar och stackutrymme, vilket kan minska prestanda och i v√§rsta fall orsaka program- eller systemkrascher p√• grund av resursbrist.

97. **Varf√∂r kan det vara sv√•rt att parallelisera rekursiva algoritmer?**
    Att parallelisera rekursiva algoritmer kan vara sv√•rt p√• grund av beroenden mellan rekursiva anrop som m√•ste l√∂sas eller hanteras korrekt. Dessutom kan den √∂verh√§ngande kostnaden f√∂r att hantera tr√•dar och synkronisering uppv√§ga de potentiella prestandaf√∂rdelarna, speciellt f√∂r sm√• problemstorlekar eller djupt rekursiva funktioner.

98. **Vad √§r en barrier**
    En barrier √§r en synkroniseringsmekanism som anv√§nds i parallell programmering f√∂r att stoppa alla involverade tr√•dar eller processer vid en viss punkt tills alla har n√•tt denna punkt. Det s√§kerst√§ller att ingen tr√•d forts√§tter f√∂rr√§n alla tr√•dar har n√•tt bari√§ren och √§r redo att forts√§tta tillsammans.

99. **Hur kan en semafor anv√§ndas f√∂r att signalera mellan tr√•dar?**
    En semafor kan anv√§ndas f√∂r att signalera mellan tr√•dar genom att en tr√•d v√§ntar (kallar `wait` p√• semaforen) p√• en signal och en annan tr√•d skickar denna signal (kallar `signal` eller `post` p√• semaforen). Detta m√∂jligg√∂r synkronisering mellan tr√•dar, d√§r en tr√•d kan v√§nta p√• att en annan ska slutf√∂ra en uppgift innan den forts√§tter.
100. **F√∂rklara hur depth-first search (DFS) kan paralleliseras med tr√•dar**
    DFS kan paralleliseras genom att tilldela olika grenar av s√∂ktr√§det till olika tr√•dar. N√§r en tr√•d n√•r en nod som har outredda barn, kan den skapa nya tr√•dar f√∂r att utforska dessa barn parallellt. Det √§r viktigt att synkronisera √•tkomsten till gemensamma resurser, som den delade datan eller resultatuppsamlingen, f√∂r att undvika inkonsekvenser.

101. **F√∂rklara hur breadth-first search (BFS) kan paralleliseras med tr√•dar**
    BFS kan paralleliseras genom att anv√§nda en gemensam k√∂ som h√•ller p√• noderna som ska utforskas. Tr√•dar kan ta bort noder fr√•n k√∂n och utforska dem samtidigt. N√§r en tr√•d utforskar en nod, l√§gger den dess barn i k√∂n. Synkronisering av k√∂n √§r n√∂dv√§ndig f√∂r att f√∂rhindra race conditions. Denna metod kan sprida ut arbetsbelastningen j√§mnt mellan tr√•darna och effektivisera s√∂kningen √∂ver en graf.

102. **F√∂rklara hur Prim‚Äôs algoritm kan paralleliseras med tr√•dar**
    Prim‚Äôs algoritm f√∂r att hitta ett minimalt sp√§nnande tr√§d i en graf kan paralleliseras genom att dela upp grafen i sektioner och l√•ta varje tr√•d k√∂ra algoritmen p√• sin sektion. Tr√•darna m√•ste synkroniseras n√§r de v√§ljer den minsta kanten som korsar gr√§nserna mellan sektionerna. Effektiv synkronisering och delning av information mellan tr√•dar √§r avg√∂rande f√∂r att uppr√§tth√•lla algoritmens korrekthet och effektivitet.

103. **Vi kan hitta det minsta v√§rdet i en lista p√• linj√§r tid (ùëÇ(ùëÅ)). Hur l√•ng tid tar det att k√∂ra med ùëÉ processorer? Motivera.**
    Om en uppgift som tar linj√§r tid ùëÇ(ùëÅ) delas j√§mnt mellan ùëÉ processorer, och varje processor arbetar p√• sin del av uppgiften parallellt, skulle den totala tiden teoretiskt kunna reduceras till ùëÇ(ùëÅ/ùëÉ), f√∂rutsatt att arbetsbelastningen f√∂rdelas j√§mnt och overhead f√∂r kommunikation och synkronisering mellan processorer √§r f√∂rsumbar. I praktiken kan dock kommunikationskostnader och oj√§mn arbetsf√∂rdelning minska denna idealiska acceleration.

104. **Vad kr√§vs f√∂r att vi skall erh√•lla en speedup p√• ùëÉ med en algoritm som k√∂rs p√• ùëÉ processorer**
    F√∂r att uppn√• en speedup p√• ùëÉ med ùëÉ processorer kr√§vs det:
    - Att arbetsbelastningen kan delas upp effektivt i ùëÉ oberoende enheter av arbete.
    - Minimal overhead f√∂r kommunikation och synkronisering mellan processorer.
    - Att arbetsbelastningen √§r tillr√§ckligt stor f√∂r att motivera den parallella overheaden.
    - Att algoritmen och problemet inte har n√•gra inneboende sekventiella beroenden som begr√§nsar parallellisering.

105. **√Ñr det alltid snabbare att parallelisera en algoritm och k√∂ra den p√• s√• m√•nga processorer som m√∂jligt? Motivera.**
    Nej, det √§r inte alltid snabbare. Amdahls lag beskriver en gr√§ns f√∂r den teoretiska maximala speedup som kan uppn√•s genom att parallelisera en ber√§kning. √ñverhead f√∂r kommunikation och synkronisering, samt delen av algoritmen som m√•ste k√∂ras sekventiellt, begr√§nsar speedup som kan uppn√•s. Dessutom, om antalet processorer √∂kar utan att arbetsbelastningen √∂kar i motsvarande grad, kan den extra overheaden f√∂r att hantera fler processorer minska den totala prestandavinsten.
106. **F√∂rklara N-ary-s√∂kning**
    N-ary-s√∂kning √§r en generalisering av bin√§rs√∂kning d√§r data delas upp i N segment ist√§llet f√∂r tv√• vid varje steg. Vid varje iteration v√§ljer algoritmen ett av N segment baserat p√• j√§mf√∂relser, vilket reducerar s√∂komr√•det betydligt snabbare √§n bin√§rs√∂kning om N v√§ljs korrekt och data √§r l√§mpligt organiserad f√∂r en s√•dan s√∂kning.

107. **F√∂rklara hur N-ary-s√∂kning kan paralleliseras med tr√•dar**
    N-ary-s√∂kning kan paralleliseras genom att tilldela varje segment till en separat tr√•d f√∂r samtidig s√∂kning. Tr√•darna kan utforska sina tilldelade segment oberoende av varandra, vilket minskar den totala s√∂ktiden. Effektiv parallelisering kr√§ver dock noggrann hantering av tr√•dsynkronisering och minimering av overhead f√∂r tr√•dskapande och -hantering.

108. **F√∂rklara lock free**
    Lock-free programmering refererar till designen av algoritmer som garanterar systemets framsteg utan att anv√§nda traditionella l√•sningsmekanismer som mutexar eller semaforer. Dessa algoritmer undviker l√•s genom att anv√§nda atom√§ra operationer, vilket minskar risken f√∂r deadlock och kan ge b√§ttre prestanda genom att minska kontextv√§xling och v√§ntetider.

109. **F√∂rklara wait free**
    Wait-free programmering √§r en starkare form av lock-free programmering d√§r systemet inte bara garanterar framsteg utan ocks√• att varje tr√•d slutf√∂r sin operation inom ett begr√§nsat antal steg, oavsett andra tr√•dars beteende. Detta s√§kerst√§ller att systemet √§r robust mot tr√•dinterferens och ger maximal responsivitet.

110. **Vad menas med en optimistisk algoritm (med avseende p√• √∂mesidig uteslutning)?**
    En optimistisk algoritm antar att konflikter och resurskollisioner √§r s√§llsynta och hanterar dem n√§r de intr√§ffar ist√§llet f√∂r att f√∂rs√∂ka f√∂rhindra dem i f√∂rv√§g. Dessa algoritmer f√∂rs√∂ker vanligtvis genomf√∂ra operationer utan l√•sning och anv√§nder kontroller efter√•t f√∂r att verifiera att ingen konflikt har uppst√•tt. Om en konflikt uppt√§cks, √•terst√§lls och upprepas operationen.

111. **F√∂rklara compare and set**
    Compare and set (CAS) √§r en atom√§r operation som anv√§nds i flertr√•dade program f√∂r att uppn√• synkronisering utan l√•s. Operationen j√§mf√∂r v√§rdet av en minnesplats med ett givet v√§rde och, endast om de √§r lika, uppdaterar minnesplatsen med ett nytt v√§rde. Detta sker i en enda, odelbar operation som garanterar att inga andra tr√•dar kan √§ndra minnesplatsen samtidigt.

112. **Visa hur compare and set kan anv√§ndas f√∂r att implementera en bin√§r semafor**
    ```pseudo
    bool locked = false

    void lock() {
        while (!compare_and_set(&locked, false, true)) {
            // V√§nta tills l√•set blir ledigt
        }
    }

    void unlock() {
        locked = false
    }
    ```
    Denna pseudo-kod anv√§nder en `compare_and_set` operation f√∂r att implementera en enkel l√•smekanism. `lock`-funktionen loopar tills den lyckas √§ndra `locked` fr√•n `false` till `true`, vilket indikerar att l√•set har f√∂rv√§rvats. `unlock`-funktionen s√§tter sedan enkelt tillbaka `locked` till `false`.

113. **F√∂rklara hand over hand locking**
    Hand over hand locking, √§ven k√§nd som lock coupling, √§r en teknik som anv√§nds vid traversering av l√§nkade datastrukturer, som l√§nkade listor eller tr√§d, d√§r varje nod l√•ses i tur och ordning. N√§r en tr√•d r√∂r sig fr√•n en nod till n√§sta, l√•ser den n√§sta nod innan den frig√∂r l√•set p√• den aktuella noden. Detta minimerar den l√•sta regionen till tv√• noder √•t g√•ngen och f√∂rb√§ttrar parallell tillg√•ng.

114. **Vad √§r nackdelarna med att l√•sa ‚Äúf√∂r mycket‚Äù?**
    Att l√•sa f√∂r mycket kan leda till minskad parallellitet och systemprestanda, √∂kad v√§ntetid f√∂r tr√•dar, risk f√∂r deadlock och on√∂digt resursutnyttjande. √ñveranv√§ndning av l√•s kan skapa flaskhalsar d√§r tr√•dar blockeras on√∂digt l√§nge, vilket minskar den totala effektiviteten i programmet.

115. **Vad √§r nackdelarna med att l√•sa ‚Äúf√∂r lite‚Äù?**
    Att l√•sa f√∂r lite kan leda till race conditions, inkonsistenta data och sv√•rdebuggade problem. Utan tillr√§cklig synkronisering kan tr√•dar samtidigt modifiera delade resurser p√• ett s√§tt som bryter mot programmets logik och f√∂rv√§ntningar, vilket resulterar i felaktiga resultat eller krascher.

116. **Vad √§r f√∂rdelarna med en optimistisk algoritm (med avseende p√• l√•sning)?**
    F√∂rdelarna med optimistiska algoritmer inkluderar h√∂gre prestanda under l√•g och m√•ttlig belastning genom att undvika l√•skostnader, minskad risk f√∂r deadlock eftersom de inte h√•ller kvar l√•s under operationer, och b√§ttre skalbarhet i milj√∂er med m√•nga processorer genom att minska behovet av synkronisering.

117. **Vad √§r nackdelarna med en optimistisk algoritm (med avseende p√• l√•sning)?**
    Nackdelarna inkluderar potentialen f√∂r √∂kad komplexitet i algoritmimplementationen, risk f√∂r att arbete m√•ste g√∂ras om om konflikter uppt√§cks, och m√∂jlig minskning av prestanda under h√∂g belastning p√• grund av omg√∂rningar. Optimistiska algoritmer kan ocks√• kr√§va noggrann utformning f√∂r att effektivt hantera konflikter.
118. **F√∂rklara hur radering i en l√§nkad lista kan implementeras med en optimistisk algoritm**
    Vid radering i en l√§nkad lista med en optimistisk algoritm, f√∂rs√∂ker man f√∂rst genomf√∂ra raderingen utan att l√•sa hela listan. Ist√§llet kontrollerar man efter√•t om operationen var korrekt med avseende p√• listans √∂vriga struktur. Detta kan inneb√§ra att man tempor√§rt l√•ser enskilda noder snarare √§n hela listan. Om en inkonsekvens uppt√§cks, √•terst√§lls operationen och f√∂rs√∂ks igen. Detta tillv√§gag√•ngss√§tt minskar l√•sningens √∂verhead men kr√§ver noggranna kontroller f√∂r att uppr√§tth√•lla dataintegriteten.

119. **Ange n√•gra problem med att skriva flertr√•dade program**
    - **Race conditions:** N√§r flera tr√•dar f√∂rs√∂ker √§ndra delade data samtidigt utan tillr√§cklig synkronisering.
    - **Deadlocks:** N√§r tv√• eller flera tr√•dar v√§ntar p√• varandra f√∂r att frig√∂ra resurser, vilket skapar en o√§ndlig v√§ntecykel.
    - **Starvation:** N√§r en eller flera tr√•dar inte f√•r tillr√§ckligt med processortid eller resurser f√∂r att utf√∂ra sina uppgifter.
    - **Livelocks:** N√§r tr√•dar √§r upptagna med att reagera p√• varandra p√• ett s√§tt som hindrar dem fr√•n att g√∂ra framsteg.
    - **√ñverhead f√∂r synkronisering:** Att hantera tr√•ds√§kerhet kan l√§gga till komplexitet och minska prestanda.

120. **Varf√∂r kan det vara sv√•rt att s√§tta samman (compose) flera flertr√•dade funktioner?**
    Att kombinera flertr√•dade funktioner kan vara sv√•rt p√• grund av √∂kad risk f√∂r deadlocks, race conditions och andra synkroniseringsproblem. Funktioner som fungerar korrekt isolerat kan interagera ov√§ntat n√§r de kombineras, s√§rskilt om de delar resurser eller om deras synkroniseringsmekanismer inte √§r kompatibla. Detta kr√§ver noggrann design och testning f√∂r att s√§kerst√§lla korrekt samverkan.

121. **Vad √§r en Future?**
    En Future √§r en abstraktion som representerar ett resultat som √§nnu inte finns tillg√§ngligt men som kommer att bli det i framtiden. Futures anv√§nds ofta i asynkron programmering f√∂r att hantera resultat fr√•n parallella eller asynkrona operationer, och de till√•ter kod att forts√§tta att k√∂ra medan v√§ntar p√• att operationen slutf√∂rs.

122. **F√∂rklara begreppet coroutine**
    Coroutines √§r programkomponenter som generaliserar subrutiner f√∂r att till√•ta flera ing√•ngspunkter f√∂r suspendering och √•terupptagande av utf√∂rande vid vissa punkter. Detta m√∂jligg√∂r asynkrona operationer p√• ett mer naturligt och effektivt s√§tt √§n traditionella callbacks, genom att till√•ta sekventiell kodstruktur f√∂r asynkrona operationer.

123. **F√∂rklara begreppet cooperative multitasking**
    Cooperative multitasking √§r en typ av multitasking d√§r processer eller tr√•dar frivilligt √∂verl√§mnar kontrollen tillbaka till schemal√§ggaren. Detta skiljer sig fr√•n preemptive multitasking d√§r schemal√§ggaren tvingar processen att ge ifr√•n sig kontrollen. I cooperative multitasking m√•ste varje process aktivt best√§mma n√§r den ska "pausa" sin exekvering, vilket minskar behovet av komplexa synkroniseringsmekanismer men kr√§ver noggrann design f√∂r att undvika att en process monopoliserar processortiden.

124. **F√∂rklara begreppet asynchronous programming**
    Asynchronous programming √§r en programmeringsmodell som till√•ter operationer, speciellt I/O-bound operationer, att k√∂ras utan att blockera programmets huvudfl√∂de. Detta uppn√•s genom att initiera operationer som kommer att slutf√∂ras i framtiden, medan programmet forts√§tter att k√∂ra annan kod. Callbacks, promises, futures och coroutines √§r vanliga konstruktioner som anv√§nds f√∂r att hantera asynkrona operationer.

125. **F√∂rklara begreppet callback**
    En callback √§r en funktion som skickas som ett argument till en annan funktion och som √§r avsedd att kallas vid ett senare tillf√§lle, oftast som svar p√• en viss h√§ndelse eller n√§r en asynkron operation √§r slutf√∂rd. Callbacks anv√§nds f√∂r att hantera asynkrona h√§ndelser eller f√∂r att anpassa beteendet hos generiska funktioner.

126. **Vad h√§nder om en blockerade funktion k√∂rs av en funktion p√• event loop**
    Om en blockerande funktion k√∂rs p√• en event loop, kan det blockera hela event-loopen fr√•n att behandla andra h√§ndelser eller utf√∂ra andra asynkrona operationer. Detta kan leda till att programmet blir otillg√§ngligt eller l√•ngsamt, eftersom inga andra √•tg√§rder kan utf√∂ras medan den blockerande funktionen k√∂rs.

127. **N√§r b√∂r man anv√§nda asynchronous programming?**
    Asynkron programmering b√∂r anv√§ndas n√§r man hanterar I/O-operationer, n√§tverksf√∂rfr√•gningar, databas√•tg√§rder eller andra operationer som kan ta tid att slutf√∂ra och d√§r man inte vill blockera programmet fr√•n att forts√§tta med andra uppgifter. Det √§r s√§rskilt anv√§ndbart i GUI-applikationer, webbservrar och i situationer d√§r man vill f√∂rb√§ttra programmets responsivitet eller prestanda.

128. **Vad √§r en kanal i Go?**
    En kanal i Go √§r en typ som anv√§nds f√∂r att m√∂jligg√∂ra s√§ker kommunikation och synkronisering mellan goroutines. Kanaler till√•ter att v√§rden skickas fr√•n en goroutine till en annan, vilket underl√§ttar samordning och utbyte av data i konkurrenta program.

129. **Vad g√∂r select i Go?**
    `select`-satsen i Go l√•ter en goroutine v√§nta p√• flera kommunikationsoperationer, inklusive kanals√§ndningar och mottagningar, att bli redo. `select` blockerar tills n√•gon av dess fall kan utf√∂ras, varvid det fallet exekveras. Om flera fall √§r redo, v√§ljs ett slumpm√§ssigt.

130. **F√∂rklara begreppet done-kanal**
    En done-kanal anv√§nds i Go f√∂r att signalera till en eller flera goroutines att avsluta sitt arbete. Detta m√∂nster anv√§nds f√∂r att rent och s√§kert avbryta p√•g√•ende operationer eller f√∂r att rensa upp resurser n√§r ett arbete √§r klart eller programmet avslutas.

131. **Visa (i Go-liknande kod) hur en goroutine kan avslutas med hj√§lp av en done-kanal**
    ```go
    package main

    import (
        "fmt"
        "time"
    )

    func worker(done chan bool) {
        fmt.Println("Working...")
        time.Sleep(time.Second)
        fmt.Println("Done")

        // Signalera att arbetet √§r klart
        done <- true
    }

    func main() {
        done := make(chan bool, 1)
        go worker(done)

        // V√§nta p√• att arbetet ska bli klart
        <-done
    }
    ```

132. **Implementera en funktion i Go som best√§mmer det st√∂rsta v√§rdet som skickas p√• en kanal**
    ```go
    package main

    import (
        "fmt"
    )

    func findMax(values chan int, maxFound chan int) {
        max := <-values // Startv√§rde
        for v := range values {
            if v > max {
                max = v
            }
        }
        maxFound <- max
    }

    func main() {
        values := make(chan int)
        maxFound := make(chan int)

        go findMax(values, maxFound)

        // Skicka v√§rden
        values <- 3
        values <- 6
        values <- 2
        values <- 8
        values <- 4
        close(values) // St√§ng kanalen f√∂r att indikera att inga fler v√§rden kommer

        // Ta emot det st√∂rsta v√§rdet
        max := <-maxFound
        fmt.Println("Det st√∂rsta v√§rdet √§r:", max)
    }
    ```

133. **Implementera en funktion i Go som delar en kanal i flera kanaler (multiplex)**
    ```go
    package main

    import (
        "fmt"
    )

    // multiplex tar en inputkanal och delar upp dess inneh√•ll i tv√• separata outputkanaler.
    func multiplex(input chan int) (chan int, chan int) {
        out1 := make(chan int)
        out2 := make(chan int)

        go func() {
            for val := range input {
                out1 <- val
                out2 <- val
            }
            close(out1)
            close(out2)
        }()

        return out1, out2
    }

    func main() {
        input := make(chan int)
        out1, out2 := multiplex(input)

        // Skicka data till inputkanalen
        go func() {
            for i := 0; i < 5; i++ {
                input <- i
            }
            close(input)
        }()

        // Ta emot data fr√•n b√•da outputkanalerna
        for i := 0; i < 5; i++ {
            fmt.Println(<-out1, <-out2)
        }
    }
    ```

134. **Vad √§r ett closure i Go?**
    Ett closure i Go √§r en funktion som kan f√•nga upp och anv√§nda variabler fr√•n den omgivande kontexten d√§r den √§r definierad. Closures √§r anv√§ndbara f√∂r att skapa funktioner p√• flygten som beh√•ller tillst√•nd mellan exekveringar eller f√∂r att skapa privata dataomr√•den.

135. **F√∂rklara WaitGroup i Go**
    En `WaitGroup` i Go anv√§nds f√∂r att synkronisera arbetet mellan flera goroutines. Den till√•ter ett program att v√§nta tills en upps√§ttning goroutines har slutf√∂rt sitt arbete. Man anropar `Add` f√∂r att s√§tta antalet goroutines att v√§nta p√•, `Done` inuti varje goroutine f√∂r att signalera att den √§r klar, och `Wait` f√∂r att blockera tills alla goroutines har anropat `Done`.
